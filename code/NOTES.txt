So the vertexes stay the same, it's not changing the vertex buffer
it's only changing the Constants buffer,
and when used in the shader, it's adding an offset, which comes from the Constants buffer.

So to add a static square, I need to make a new shader that doesn't add an offset?
no.. I can just set a new constants buffer that has an offset of 0.

Ok, got that working. But that's just a workaround to reuse the same shader.
To actually draw a simple block background, it should have it's own basic shader.

Hm. The vertex shader is supposed to be responsible for getting the coordinates into clip space
so that's probably where I can do conversions
I can have all my world in my own coordinates, and have the shader convert it to [-1,1]




5/7 End of day notes:

I have many questions on how to set this all up.
My initial thoughts

I have some render struct thing
when i output some render struct thing, I throw it in a buffer for d3d to read
then in the vertex shader, i convert the vertices into clip space like it requires
then profit?

but like, what buffers need to be updating often vs init?
	The long post was kinda good for this, explaining 3 different constant buffers
	based on how often they are changing

	so really, just continue reading that post

I stopped reading right around "DirectX Demo Cont..." and "Load Demo Content"

5/9 sunday experimenting:

Q: Can I just set a Z value and have it Z-order my two shapes in front/behind automatically?
A: No, Setting Z does not automatically tell it the order, it still depends on the draw order.

Q: How do I have my own world coordinates? and have the Vertex buffer convert them?
First test, setting vertex data outside of [-1,1] range caused the square to take the entire screen
A: Basically a bunch of matrix math.
"Coords output by the vertex shader have the projection matrix applied, but not the perspective divide."
Basically this (in the vertex shader) (variables set up in a constants buffer):
	output.position = mul(input.position, worldMatrix);
	output.position = mul(output.position, viewMatrix);
	output.position = mul(output.position, projectionMatrix);

This questions is actually much more complicated than I thought
It's not only this matrix math between the Model View Projection (MVP)
but I think the vertex shader automatically goes from xyzw to xyz by dividing out the w,
thus finally converting it to normalized device coordinates.
The w is usually 1, but if it's considered with the projection (camera?) then it's possibly not 1
(this division is _perspective division_)
This article was ok. https://jsantell.com/model-view-projection/
